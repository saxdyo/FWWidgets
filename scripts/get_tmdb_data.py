import os
import json
import requests
from datetime import datetime, timezone, timedelta
import pytz
from bs4 import BeautifulSoup
import time
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# é…ç½®
TMDB_API_KEY = os.getenv("TMDB_API_KEY")
BASE_URL = "https://api.themoviedb.org/3"
SAVE_PATH = os.path.join(os.getcwd(), "data", "TMDB_Trending.json")

# ä½¿ç”¨pytzå¤„ç†æ—¶åŒº
BEIJING_TZ = pytz.timezone('Asia/Shanghai')

def safe_request(url, params=None, timeout=10, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„å®‰å…¨è¯·æ±‚å‡½æ•°"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, params=params, timeout=timeout)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
            else:
                logger.error(f"æ‰€æœ‰é‡è¯•å¤±è´¥: {url}")
                raise

def fetch_tmdb_data(time_window="day", media_type="all"):
    """è·å–TMDBçƒ­é—¨æ•°æ®"""
    if not TMDB_API_KEY:
        logger.warning("TMDB_API_KEYæœªè®¾ç½®")
        return {"results": []}

    endpoint = f"/trending/all/{time_window}" if media_type == "all" else f"/trending/{media_type}/{time_window}"
    url = f"{BASE_URL}{endpoint}"
    params = {"api_key": TMDB_API_KEY, "language": "zh-CN"}

    return safe_request(url, params)

def fetch_popular_movies():  
    """è·å–çƒ­é—¨ç”µå½±"""
    if not TMDB_API_KEY:
        return {"results": []}
    
    endpoint = "/movie/popular"
    url = f"{BASE_URL}{endpoint}"
    params = {
        "api_key": TMDB_API_KEY,
        "language": "zh-CN",
        "region": "CN",
        "page": 1
    }
    
    data = safe_request(url, params)
    data["results"] = data["results"][:15]  # é™åˆ¶è¿”å›15ä¸ª
    return data

def fetch_top_rated_content(media_type="movie"):
    """è·å–é«˜åˆ†å†…å®¹"""
    if not TMDB_API_KEY:
        return {"results": []}
    
    endpoint = f"/{media_type}/top_rated"
    url = f"{BASE_URL}{endpoint}"
    params = {
        "api_key": TMDB_API_KEY,
        "language": "zh-CN",
        "page": 1
    }
    
    data = safe_request(url, params)
    data["results"] = data["results"][:10]  # é™åˆ¶è¿”å›10ä¸ª
    return data

def get_media_details(media_type, media_id):
    """è·å–åª’ä½“è¯¦ç»†ä¿¡æ¯"""
    if not TMDB_API_KEY:
        return {"genres": []}
    
    detail_endpoint = f"/{media_type}/{media_id}"
    url = f"{BASE_URL}{detail_endpoint}"
    params = {"api_key": TMDB_API_KEY, "language": "zh-CN"}
    
    return safe_request(url, params)

def get_media_images(media_type, media_id):
    """è·å–åª’ä½“å›¾ç‰‡ä¿¡æ¯"""
    images_endpoint = f"/{media_type}/{media_id}/images"
    url = f"{BASE_URL}{images_endpoint}"
    params = {
        "api_key": TMDB_API_KEY,
        "include_image_language": "zh,en,null"
    }
    return safe_request(url, params)

def get_image_url(path, size="original"):
    """æ„å»ºå›¾ç‰‡URL"""
    if not path:
        return None
    return f"https://image.tmdb.org/t/p/{size}{path}"

def get_best_title_backdrop(image_data):
    """æ™ºèƒ½é€‰æ‹©æœ€ä½³èƒŒæ™¯å›¾"""
    backdrops = image_data.get("backdrops", [])
    
    if not backdrops:
        return None
    
    def get_priority_score(backdrop):
        # è¯­è¨€ä¼˜å…ˆçº§ï¼šä¸­æ–‡ > è‹±æ–‡ > æ— è¯­è¨€ > å…¶ä»–
        lang = backdrop.get("iso_639_1")
        if lang == "zh":
            lang_score = 0
        elif lang == "en":
            lang_score = 1
        elif lang is None:
            lang_score = 2
        else:
            lang_score = 3
        
        # è¯„åˆ†ï¼ˆå–è´Ÿå€¼ç”¨äºå‡åºæ’åˆ—ï¼‰
        vote_avg = -backdrop.get("vote_average", 0)
        
        # åˆ†è¾¨ç‡ï¼ˆå–è´Ÿå€¼ç”¨äºå‡åºæ’åˆ—ï¼‰
        width = backdrop.get("width", 0)
        height = backdrop.get("height", 0)
        resolution = -(width * height)
        
        return (lang_score, vote_avg, resolution)
    
    sorted_backdrops = sorted(backdrops, key=get_priority_score)
    best_backdrop = sorted_backdrops[0]
    return get_image_url(best_backdrop["file_path"])

def generate_title_backdrop_url(item, item_type, backdrop_path):
    """ç”Ÿæˆå¸¦æ ‡é¢˜çš„èƒŒæ™¯å›¾URLï¼Œä½¿ç”¨å¯ç”¨çš„OGå›¾ç‰‡æœåŠ¡"""
    if not backdrop_path:
        return None
        
    title = item.get("title") or item.get("name", "")
    release_date = item.get("release_date") or item.get("first_air_date", "")
    rating = item.get("vote_average", 0)
    
    # æå–å¹´ä»½
    year = ""
    if release_date:
        try:
            year = release_date.split("-")[0]
        except:
            year = ""
    
    # å¦‚æœç¼ºå°‘å¿…è¦ä¿¡æ¯ï¼Œè¿”å›åŸå§‹èƒŒæ™¯å›¾
    if not title or not year:
        return get_image_url(backdrop_path, "w1280")
    
    # æ„å»ºæ ‡é¢˜èƒŒæ™¯å›¾URLï¼Œä½¿ç”¨å¯ç”¨çš„OGæœåŠ¡
    try:
        from urllib.parse import quote
        
        # æ„å»ºå‰¯æ ‡é¢˜
        subtitle = f"{year} â€¢ â­ {rating:.1f} â€¢ {item_type}"
        
        # æš‚æ—¶ä¸ç”Ÿæˆå¸¦æ ‡é¢˜èƒŒæ™¯å›¾ï¼Œé¿å…404é”™è¯¯
        # ç›´æ¥è¿”å›åŸå§‹èƒŒæ™¯å›¾
        logger.info(f"ğŸ”„ è·³è¿‡æ ‡é¢˜èƒŒæ™¯å›¾ç”Ÿæˆï¼Œä½¿ç”¨åŸå§‹èƒŒæ™¯å›¾: {title}")
        return get_image_url(backdrop_path, "w1280")
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ ‡é¢˜èƒŒæ™¯å›¾å¤±è´¥: {e}")
        # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œè¿”å›åŸå§‹èƒŒæ™¯å›¾
        return get_image_url(backdrop_path, "w1280")

def enhance_with_external_data(item, item_type):
    """ä½¿ç”¨BeautifulSoupå¢å¼ºæ•°æ®ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
    # è¿™æ˜¯ä¸€ä¸ªæ‰©å±•åŠŸèƒ½ç¤ºä¾‹ï¼Œå¯ä»¥çˆ¬å–é¢å¤–ä¿¡æ¯
    # å½“å‰è¿”å›åŸæ•°æ®ï¼Œæœªæ¥å¯ä»¥æ·»åŠ æ›´å¤šæ•°æ®æº
    return item

def process_tmdb_data(data, time_window, media_type, section_name=""):
    """å¤„ç†TMDBæ•°æ®"""
    results = []
    logger.info(f"å¼€å§‹å¤„ç†{section_name}æ•°æ®ï¼ŒåŸå§‹æ•°æ®é‡: {len(data.get('results', []))}")
    
    for item in data.get("results", []):
        try:
            title = item.get("title") or item.get("name")
            item_type = media_type if media_type != "all" else item.get("media_type")
            
            # è·³è¿‡äººç‰©ç±»å‹
            if item_type == "person":
                continue
            
            # è·å–å‘å¸ƒæ—¥æœŸ
            if item_type == "tv":
                release_date = item.get("first_air_date")
            else:
                release_date = item.get("release_date")
            
            overview = item.get("overview", "")
            rating = round(item.get("vote_average", 0), 1)
            media_id = item.get("id")
            popularity = item.get("popularity", 0)
            vote_count = item.get("vote_count", 0)

            # è·å–æµ·æŠ¥URL
            poster_url = get_image_url(item.get("poster_path"))

            # è·å–è¯¦ç»†ä¿¡æ¯å’Œç±»å‹
            try:
                detail_data = get_media_details(item_type, media_id)
                genres = detail_data.get("genres", [])
                genre_title = "â€¢".join([g["name"] for g in genres[:3]])
            except Exception as e:
                logger.warning(f"è·å–è¯¦ç»†ä¿¡æ¯å¤±è´¥ {title}: {e}")
                genre_title = ""

            # ç”Ÿæˆå¸¦æ ‡é¢˜çš„èƒŒæ™¯å›¾
            try:
                # ä¼˜å…ˆä½¿ç”¨æˆ‘ä»¬çš„æ ‡é¢˜èƒŒæ™¯å›¾ç”ŸæˆæœåŠ¡
                if item.get("backdrop_path"):
                    title_backdrop_url = generate_title_backdrop_url(item, item_type, item.get("backdrop_path"))
                else:
                    # å¦‚æœæ²¡æœ‰backdrop_pathï¼Œå°è¯•è·å–å›¾ç‰‡æ•°æ®
                    image_data = get_media_images(item_type, media_id)
                    best_backdrop = get_best_title_backdrop(image_data)
                    if best_backdrop:
                        # ä»æœ€ä½³èƒŒæ™¯å›¾æå–è·¯å¾„å¹¶ç”Ÿæˆæ ‡é¢˜èƒŒæ™¯å›¾
                        backdrop_path = best_backdrop.replace("https://image.tmdb.org/t/p/original", "")
                        title_backdrop_url = generate_title_backdrop_url(item, item_type, backdrop_path)
                    else:
                        title_backdrop_url = None
                    
            except Exception as e:
                logger.warning(f"ç”Ÿæˆæ ‡é¢˜èƒŒæ™¯å›¾å¤±è´¥ {title}: {e}")
                # å›é€€åˆ°åŸå§‹èƒŒæ™¯å›¾
                title_backdrop_url = get_image_url(item.get("backdrop_path")) if item.get("backdrop_path") else None

            # æ•°æ®è´¨é‡æ£€æŸ¥
            if (rating == 0 and 
                not release_date and 
                not overview and 
                not poster_url):
                logger.debug(f"è·³è¿‡ä½è´¨é‡æ•°æ®: {title}")
                continue

            # å¢å¼ºæ•°æ®ï¼ˆå¯é€‰ï¼‰
            enhanced_item = enhance_with_external_data(item, item_type)

            result_item = {
                "id": media_id,
                "title": title,
                "type": item_type,
                "genreTitle": genre_title,
                "rating": rating,
                "release_date": release_date,
                "overview": overview,
                "poster_url": poster_url,
                "title_backdrop": title_backdrop_url,
                "popularity": popularity,
                "vote_count": vote_count
            }
            
            results.append(result_item)
            
        except Exception as e:
            logger.error(f"å¤„ç†é¡¹ç›®å¤±è´¥ {item.get('title', 'Unknown')}: {e}")
            continue
    
    logger.info(f"{section_name}æ•°æ®å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ•°æ®é‡: {len(results)}")
    return results

def save_to_json(data, filepath):
    """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        raise

def print_trending_results(results, section_title):
    """æ‰“å°ç»“æœæ‘˜è¦"""
    print(f"\n================= {section_title} =================")
    
    for i, item in enumerate(results[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
        title = item.get("title", "æœªçŸ¥æ ‡é¢˜")
        item_type = item.get("type", "unknown")
        rating = item.get("rating", 0)
        genre_title = item.get("genreTitle", "")
        
        print(f"{i:2d}. {title} ({item_type}) è¯„åˆ†: {rating} | {genre_title}")
    
    if len(results) > 10:
        print(f"    ... è¿˜æœ‰ {len(results) - 10} ä¸ªé¡¹ç›®")

def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´"""
    return datetime.now(BEIJING_TZ).strftime("%Y-%m-%d %H:%M:%S")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=== å¼€å§‹æ‰§è¡ŒTMDBæ•°æ®è·å– ===")
    
    if not TMDB_API_KEY:
        logger.warning("TMDB_API_KEYæœªè®¾ç½®ï¼Œç”Ÿæˆç©ºæ•°æ®æ–‡ä»¶")
        
        data_to_save = {
            "last_updated": get_beijing_time(),
            "today_global": [],
            "week_global_all": [],
            "popular_movies": [],
            "top_rated_movies": [],
            "status": "no_api_key"
        }
        save_to_json(data_to_save, SAVE_PATH)
        print("================= æ‰§è¡Œå®Œæˆ =================")
        return

    try:
        # è·å–å„ç±»æ•°æ®
        logger.info("å¼€å§‹è·å–ä»Šæ—¥çƒ­é—¨æ•°æ®...")
        today_global = fetch_tmdb_data(time_window="day", media_type="all")
        today_processed = process_tmdb_data(today_global, "day", "all", "ä»Šæ—¥çƒ­é—¨")

        logger.info("å¼€å§‹è·å–æœ¬å‘¨çƒ­é—¨æ•°æ®...")
        week_global_all = fetch_tmdb_data(time_window="week", media_type="all")
        week_processed = process_tmdb_data(week_global_all, "week", "all", "æœ¬å‘¨çƒ­é—¨")

        logger.info("å¼€å§‹è·å–çƒ­é—¨ç”µå½±æ•°æ®...")
        popular_movies = fetch_popular_movies()
        popular_processed = process_tmdb_data(popular_movies, "popular", "movie", "çƒ­é—¨ç”µå½±")

        logger.info("å¼€å§‹è·å–é«˜åˆ†ç”µå½±æ•°æ®...")
        top_rated_movies = fetch_top_rated_content("movie")
        top_rated_processed = process_tmdb_data(top_rated_movies, "top_rated", "movie", "é«˜åˆ†ç”µå½±")

        # è·å–æ›´æ–°æ—¶é—´
        last_updated = get_beijing_time()
        logger.info(f"æ•°æ®è·å–å®Œæˆï¼Œæ›´æ–°æ—¶é—´: {last_updated}")

        # æ‰“å°ç»“æœæ‘˜è¦
        print_trending_results(today_processed, "ä»Šæ—¥çƒ­é—¨")
        print_trending_results(week_processed, "æœ¬å‘¨çƒ­é—¨")
        print_trending_results(popular_processed, "çƒ­é—¨ç”µå½±")
        print_trending_results(top_rated_processed, "é«˜åˆ†ç”µå½±")

        # æ„å»ºæœ€ç»ˆæ•°æ®
        data_to_save = {
            "last_updated": last_updated,
            "today_global": today_processed,
            "week_global_all": week_processed,
            "popular_movies": popular_processed,
            "top_rated_movies": top_rated_processed,
            "status": "success",
            "total_items": len(today_processed) + len(week_processed) + len(popular_processed) + len(top_rated_processed)
        }

        # ä¿å­˜æ•°æ®
        save_to_json(data_to_save, SAVE_PATH)
        
        logger.info("================= æ‰§è¡Œå®Œæˆ =================")
        print(f"\nâœ… æˆåŠŸè·å–å¹¶ä¿å­˜äº† {data_to_save['total_items']} ä¸ªé¡¹ç›®çš„æ•°æ®")

    except Exception as e:
        logger.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        
        # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜ä¸€ä¸ªåŸºæœ¬æ–‡ä»¶
        error_data = {
            "last_updated": get_beijing_time(),
            "today_global": [],
            "week_global_all": [],
            "popular_movies": [],
            "top_rated_movies": [],
            "status": "error",
            "error_message": str(e)
        }
        save_to_json(error_data, SAVE_PATH)
        raise

if __name__ == "__main__":
    main()
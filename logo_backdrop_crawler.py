import os
import json
import requests
import time
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re

class LogoBackdropCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.base_path = os.path.join(os.getcwd(), "data")
        self.logo_data_path = os.path.join(self.base_path, "logo_backdrops.json")
        
    def get_logo_sources(self):
        """è·å–logoèƒŒæ™¯å›¾çš„æ¥æºç½‘ç«™"""
        return {
            "netflix": {
                "url": "https://www.netflix.com/",
                "logo_pattern": r"netflix.*logo",
                "backdrop_pattern": r"hero.*image|banner.*image"
            },
            "disney": {
                "url": "https://www.disney.com/",
                "logo_pattern": r"disney.*logo",
                "backdrop_pattern": r"hero.*image|banner.*image"
            },
            "hbo": {
                "url": "https://www.hbo.com/",
                "logo_pattern": r"hbo.*logo",
                "backdrop_pattern": r"hero.*image|banner.*image"
            },
            "amazon": {
                "url": "https://www.amazon.com/",
                "logo_pattern": r"amazon.*logo",
                "backdrop_pattern": r"hero.*image|banner.*image"
            },
            "hulu": {
                "url": "https://www.hulu.com/",
                "logo_pattern": r"hulu.*logo",
                "backdrop_pattern": r"hero.*image|banner.*image"
            }
        }
    
    def crawl_website_logos(self, website_name, website_config):
        """çˆ¬å–ç‰¹å®šç½‘ç«™çš„logoå’ŒèƒŒæ™¯å›¾"""
        try:
            print(f"æ­£åœ¨çˆ¬å– {website_name} çš„logoå’ŒèƒŒæ™¯å›¾...")
            
            response = self.session.get(website_config["url"], timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾logoå›¾ç‰‡
            logos = []
            logo_pattern = re.compile(website_config["logo_pattern"], re.IGNORECASE)
            
            # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ ‡ç­¾
            for img in soup.find_all('img'):
                src = img.get('src', '')
                alt = img.get('alt', '')
                title = img.get('title', '')
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯logo
                if (logo_pattern.search(src) or 
                    logo_pattern.search(alt) or 
                    logo_pattern.search(title)):
                    
                    logo_url = urljoin(website_config["url"], src)
                    logos.append({
                        "url": logo_url,
                        "alt": alt,
                        "title": title,
                        "width": img.get('width'),
                        "height": img.get('height')
                    })
            
            # æŸ¥æ‰¾èƒŒæ™¯å›¾
            backdrops = []
            backdrop_pattern = re.compile(website_config["backdrop_pattern"], re.IGNORECASE)
            
            # æŸ¥æ‰¾CSSèƒŒæ™¯å›¾
            for style in soup.find_all('style'):
                if style.string:
                    # æŸ¥æ‰¾background-imageå±æ€§
                    bg_matches = re.findall(r'background-image:\s*url\(["\']?([^"\')\s]+)["\']?\)', style.string)
                    for bg_url in bg_matches:
                        if backdrop_pattern.search(bg_url):
                            backdrop_url = urljoin(website_config["url"], bg_url)
                            backdrops.append({
                                "url": backdrop_url,
                                "type": "css_background",
                                "source": "style_tag"
                            })
            
            # æŸ¥æ‰¾å†…è”æ ·å¼ä¸­çš„èƒŒæ™¯å›¾
            for element in soup.find_all(attrs={'style': True}):
                style = element.get('style', '')
                bg_matches = re.findall(r'background-image:\s*url\(["\']?([^"\')\s]+)["\']?\)', style)
                for bg_url in bg_matches:
                    if backdrop_pattern.search(bg_url):
                        backdrop_url = urljoin(website_config["url"], bg_url)
                        backdrops.append({
                            "url": backdrop_url,
                            "type": "inline_background",
                            "source": "inline_style"
                        })
            
            return {
                "website": website_name,
                "url": website_config["url"],
                "logos": logos,
                "backdrops": backdrops,
                "crawled_at": datetime.now(timezone(timedelta(hours=8))).isoformat()
            }
            
        except Exception as e:
            print(f"çˆ¬å– {website_name} æ—¶å‡ºé”™: {str(e)}")
            return {
                "website": website_name,
                "url": website_config["url"],
                "logos": [],
                "backdrops": [],
                "error": str(e),
                "crawled_at": datetime.now(timezone(timedelta(hours=8))).isoformat()
            }
    
    def crawl_all_logos(self):
        """çˆ¬å–æ‰€æœ‰ç½‘ç«™çš„logoå’ŒèƒŒæ™¯å›¾"""
        print("=== å¼€å§‹çˆ¬å–logoèƒŒæ™¯å›¾ ===")
        
        sources = self.get_logo_sources()
        all_results = []
        
        for website_name, website_config in sources.items():
            result = self.crawl_website_logos(website_name, website_config)
            all_results.append(result)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
            time.sleep(2)
        
        # ä¿å­˜ç»“æœ
        self.save_logo_data(all_results)
        
        # æ‰“å°ç»“æœ
        self.print_results(all_results)
        
        return all_results
    
    def save_logo_data(self, data):
        """ä¿å­˜logoæ•°æ®åˆ°JSONæ–‡ä»¶"""
        os.makedirs(self.base_path, exist_ok=True)
        
        save_data = {
            "last_updated": datetime.now(timezone(timedelta(hours=8))).strftime("%Y-%m-%d %H:%M:%S"),
            "total_websites": len(data),
            "websites": data
        }
        
        with open(self.logo_data_path, "w", encoding="utf-8") as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Logoæ•°æ®å·²ä¿å­˜åˆ°: {self.logo_data_path}")
    
    def print_results(self, results):
        """æ‰“å°çˆ¬å–ç»“æœ"""
        print("\n=== çˆ¬å–ç»“æœæ±‡æ€» ===")
        
        for result in results:
            website = result["website"]
            logos_count = len(result["logos"])
            backdrops_count = len(result["backdrops"])
            
            print(f"\nğŸ“º {website.upper()}")
            print(f"   Logoæ•°é‡: {logos_count}")
            print(f"   èƒŒæ™¯å›¾æ•°é‡: {backdrops_count}")
            
            if "error" in result:
                print(f"   âŒ é”™è¯¯: {result['error']}")
            else:
                print(f"   âœ… çˆ¬å–æˆåŠŸ")
                
                if logos_count > 0:
                    print(f"   ğŸ–¼ï¸  Logoç¤ºä¾‹:")
                    for i, logo in enumerate(result["logos"][:3]):
                        print(f"      {i+1}. {logo['url']}")
                
                if backdrops_count > 0:
                    print(f"   ğŸ¨  èƒŒæ™¯å›¾ç¤ºä¾‹:")
                    for i, backdrop in enumerate(result["backdrops"][:3]):
                        print(f"      {i+1}. {backdrop['url']}")
    
    def load_existing_data(self):
        """åŠ è½½ç°æœ‰çš„logoæ•°æ®"""
        if os.path.exists(self.logo_data_path):
            with open(self.logo_data_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return None
    
    def merge_with_tmdb_data(self, tmdb_data_path):
        """å°†logoæ•°æ®ä¸TMDBæ•°æ®åˆå¹¶"""
        if not os.path.exists(tmdb_data_path):
            print(f"âŒ TMDBæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {tmdb_data_path}")
            return
        
        # åŠ è½½TMDBæ•°æ®
        with open(tmdb_data_path, "r", encoding="utf-8") as f:
            tmdb_data = json.load(f)
        
        # åŠ è½½logoæ•°æ®
        logo_data = self.load_existing_data()
        if not logo_data:
            print("âŒ Logoæ•°æ®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œçˆ¬è™«")
            return
        
        # åˆ›å»ºlogoæ˜ å°„
        logo_mapping = {}
        for website_data in logo_data.get("websites", []):
            website_name = website_data["website"]
            if website_data.get("logos"):
                logo_mapping[website_name] = website_data["logos"][0]["url"]
            if website_data.get("backdrops"):
                logo_mapping[f"{website_name}_backdrop"] = website_data["backdrops"][0]["url"]
        
        # ä¸ºTMDBæ•°æ®æ·»åŠ logoä¿¡æ¯
        for section in ["today_global", "week_global_all", "popular_movies"]:
            if section in tmdb_data:
                for item in tmdb_data[section]:
                    # æ ¹æ®æ ‡é¢˜æˆ–ç±»å‹åŒ¹é…logo
                    title = item.get("title", "").lower()
                    media_type = item.get("type", "")
                    
                    # ç®€å•çš„logoåŒ¹é…é€»è¾‘
                    if "netflix" in title or media_type == "netflix":
                        item["platform_logo"] = logo_mapping.get("netflix", "")
                        item["platform_backdrop"] = logo_mapping.get("netflix_backdrop", "")
                    elif "disney" in title or media_type == "disney":
                        item["platform_logo"] = logo_mapping.get("disney", "")
                        item["platform_backdrop"] = logo_mapping.get("disney_backdrop", "")
                    elif "hbo" in title or media_type == "hbo":
                        item["platform_logo"] = logo_mapping.get("hbo", "")
                        item["platform_backdrop"] = logo_mapping.get("hbo_backdrop", "")
                    elif "amazon" in title or media_type == "amazon":
                        item["platform_logo"] = logo_mapping.get("amazon", "")
                        item["platform_backdrop"] = logo_mapping.get("amazon_backdrop", "")
                    elif "hulu" in title or media_type == "hulu":
                        item["platform_logo"] = logo_mapping.get("hulu", "")
                        item["platform_backdrop"] = logo_mapping.get("hulu_backdrop", "")
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        merged_path = os.path.join(self.base_path, "TMDB_Trending_with_logos.json")
        with open(merged_path, "w", encoding="utf-8") as f:
            json.dump(tmdb_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆå¹¶æ•°æ®å·²ä¿å­˜åˆ°: {merged_path}")

def main():
    crawler = LogoBackdropCrawler()
    
    # çˆ¬å–logoå’ŒèƒŒæ™¯å›¾
    results = crawler.crawl_all_logos()
    
    # åˆå¹¶TMDBæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    tmdb_data_path = os.path.join(crawler.base_path, "TMDB_Trending.json")
    if os.path.exists(tmdb_data_path):
        print("\n=== åˆå¹¶TMDBæ•°æ® ===")
        crawler.merge_with_tmdb_data(tmdb_data_path)
    
    print("\n=== çˆ¬å–å®Œæˆ ===")

if __name__ == "__main__":
    main()